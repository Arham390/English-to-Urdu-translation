{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2070891e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers datasets sentencepiece sacrebleu evaluate accelerate kaggle\n",
    "\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "\n",
    "# Create a working directory inside Drive so models/data persist\n",
    "import os\n",
    "WORKDIR = '/content/drive/MyDrive/mt_en_ur'\n",
    "os.makedirs(WORKDIR, exist_ok=True)\n",
    "print('Working dir:', WORKDIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39108b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "if os.path.exists('/content/kaggle.json'):\n",
    "  os.makedirs('/root/.kaggle', exist_ok=True)\n",
    "  shutil.copy('/content/kaggle.json','/root/.kaggle/kaggle.json')\n",
    "  os.chmod('/root/.kaggle/kaggle.json', 0o600)\n",
    "\n",
    "\n",
    "# Download the Kaggle dataset (Parallel Corpus for English-Urdu Language)\n",
    "# dataset slug: zainuddin123/parallel-corpus-for-english-urdu-language\n",
    "\n",
    "\n",
    "!kaggle datasets download -d zainuddin123/parallel-corpus-for-english-urdu-language -p {WORKDIR} --unzip\n",
    "\n",
    "\n",
    "# After download, inspect files\n",
    "!ls -la {WORKDIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8a841a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "\n",
    "# Try to find a CSV or TXT in the dataset folder\n",
    "candidates = glob.glob(WORKDIR + '/*/*') # Look inside subdirectories too\n",
    "print('candidates:', candidates)\n",
    "\n",
    "\n",
    "# Adjust the filename below if different\n",
    "# Many Kaggle uploads contain a file 'parallel-corpus-en-ur.csv' or similar\n",
    "csv_files = [p for p in candidates if p.lower().endswith('.csv')]\n",
    "if len(csv_files)==0:\n",
    "  csv_files = [p for p in candidates if p.lower().endswith('.tsv')]\n",
    "if len(csv_files)==0:\n",
    "  csv_files = [p for p in candidates if p.lower().endswith('.txt')]\n",
    "\n",
    "\n",
    "print('CSV files found:', csv_files)\n",
    "\n",
    "\n",
    "if not csv_files:\n",
    "  raise FileNotFoundError('No CSV/TSV/TXT found in dataset directory. Check files list above and update filename in cell.')\n",
    "\n",
    "\n",
    "# The text files contain one sentence per line, with English and Urdu in separate files.\n",
    "# We need to load both files and combine them into a single dataframe.\n",
    "if len(csv_files) == 2 and all(f.lower().endswith('.txt') for f in csv_files):\n",
    "    en_file = [f for f in csv_files if 'english' in f.lower()][0]\n",
    "    ur_file = [f for f in csv_files if 'urdu' in f.lower()][0]\n",
    "\n",
    "    with open(en_file, 'r', encoding='utf-8') as f:\n",
    "        en_lines = f.readlines()\n",
    "    with open(ur_file, 'r', encoding='utf-8') as f:\n",
    "        ur_lines = f.readlines()\n",
    "\n",
    "    # Create dataframe from the two lists\n",
    "    df = pd.DataFrame({'en': en_lines, 'ur': ur_lines})\n",
    "\n",
    "else:\n",
    "  # If not two text files, assume it's a single CSV/TSV with columns to identify\n",
    "  df = pd.read_csv(csv_files[0], encoding='utf-8', error_bad_lines=False)\n",
    "\n",
    "  # You need to identify which columns are English and Urdu; common names: 'english','urdu' or 'en','ur'\n",
    "  # Attempt common guesses\n",
    "  possible_src = [c for c in df.columns if 'en' in c.lower() or 'english' in c.lower()]\n",
    "  possible_tgt = [c for c in df.columns if 'ur' in c.lower() or 'urdu' in c.lower()]\n",
    "  print('possible_src', possible_src)\n",
    "  print('possible_tgt', possible_tgt)\n",
    "\n",
    "\n",
    "  src_col = possible_src[0] if possible_src else df.columns[0]\n",
    "  tgt_col = possible_tgt[0] if possible_tgt else df.columns[1]\n",
    "  print('Using columns:', src_col, tgt_col)\n",
    "\n",
    "\n",
    "  # Keep only those two columns and drop NA rows\n",
    "  df = df[[src_col, tgt_col]].dropna()\n",
    "  df.columns = ['en','ur']\n",
    "\n",
    "\n",
    "print(df.shape)\n",
    "print(df.columns)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e25f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def clean_text_en(s):\n",
    "  s = s.lower()\n",
    "  s=s.strip()\n",
    "  s = re.sub(r\"\\s+\",\" \",s)\n",
    "  return s\n",
    "\n",
    "def clean_text_ur(s):\n",
    "  s=s.lower()\n",
    "  s = str(s).strip()\n",
    "  s = re.sub(r\"\\s+\",\" \",s)\n",
    "  return s\n",
    "\n",
    "_df = df.copy()\n",
    "_df['en'] = _df['en'].apply(clean_text_en)\n",
    "_df['ur'] = _df['ur'].apply(clean_text_ur)\n",
    "\n",
    "\n",
    "# Drop empty lines\n",
    "_df = _df[(_df['en']!='') & (_df['ur']!='')]\n",
    "print('Pairs after cleaning:', len(_df))\n",
    "\n",
    "\n",
    "# Save a master file and show some examples\n",
    "_master = os.path.join(WORKDIR,'parallel_master.csv')\n",
    "_df.to_csv(_master, index=False)\n",
    "print('Saved master csv to', _master)\n",
    "_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b2949c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "train_val, test_df = train_test_split(_df, test_size=0.10, random_state=42)\n",
    "train_df, val_df = train_test_split(train_val, test_size=0.1111, random_state=42) # ~80/10/10\n",
    "\n",
    "\n",
    "print('train, val, test sizes:', len(train_df), len(val_df), len(test_df))\n",
    "\n",
    "\n",
    "for name,dfpart in [('train',train_df),('valid',val_df),('test',test_df)]:\n",
    "  en_path = os.path.join(WORKDIR,f'{name}.en')\n",
    "  ur_path = os.path.join(WORKDIR,f'{name}.ur')\n",
    "  dfpart['en'].to_csv(en_path, index=False, header=False)\n",
    "  dfpart['ur'].to_csv(ur_path, index=False, header=False)\n",
    "  print('Saved:', en_path, ur_path)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
