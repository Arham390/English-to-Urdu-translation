{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2070891e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers datasets sentencepiece sacrebleu evaluate accelerate kaggle\n",
    "\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "\n",
    "# Create a working directory inside Drive so models/data persist\n",
    "import os\n",
    "WORKDIR = '/content/drive/MyDrive/mt_en_ur'\n",
    "os.makedirs(WORKDIR, exist_ok=True)\n",
    "print('Working dir:', WORKDIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39108b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "if os.path.exists('/content/kaggle.json'):\n",
    "  os.makedirs('/root/.kaggle', exist_ok=True)\n",
    "  shutil.copy('/content/kaggle.json','/root/.kaggle/kaggle.json')\n",
    "  os.chmod('/root/.kaggle/kaggle.json', 0o600)\n",
    "\n",
    "\n",
    "# Download the Kaggle dataset (Parallel Corpus for English-Urdu Language)\n",
    "# dataset slug: zainuddin123/parallel-corpus-for-english-urdu-language\n",
    "\n",
    "\n",
    "!kaggle datasets download -d zainuddin123/parallel-corpus-for-english-urdu-language -p {WORKDIR} --unzip\n",
    "\n",
    "\n",
    "# After download, inspect files\n",
    "!ls -la {WORKDIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8a841a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "\n",
    "# Try to find a CSV or TXT in the dataset folder\n",
    "candidates = glob.glob(WORKDIR + '/*/*') # Look inside subdirectories too\n",
    "print('candidates:', candidates)\n",
    "\n",
    "\n",
    "# Adjust the filename below if different\n",
    "# Many Kaggle uploads contain a file 'parallel-corpus-en-ur.csv' or similar\n",
    "csv_files = [p for p in candidates if p.lower().endswith('.csv')]\n",
    "if len(csv_files)==0:\n",
    "  csv_files = [p for p in candidates if p.lower().endswith('.tsv')]\n",
    "if len(csv_files)==0:\n",
    "  csv_files = [p for p in candidates if p.lower().endswith('.txt')]\n",
    "\n",
    "\n",
    "print('CSV files found:', csv_files)\n",
    "\n",
    "\n",
    "if not csv_files:\n",
    "  raise FileNotFoundError('No CSV/TSV/TXT found in dataset directory. Check files list above and update filename in cell.')\n",
    "\n",
    "\n",
    "# The text files contain one sentence per line, with English and Urdu in separate files.\n",
    "# We need to load both files and combine them into a single dataframe.\n",
    "if len(csv_files) == 2 and all(f.lower().endswith('.txt') for f in csv_files):\n",
    "    en_file = [f for f in csv_files if 'english' in f.lower()][0]\n",
    "    ur_file = [f for f in csv_files if 'urdu' in f.lower()][0]\n",
    "\n",
    "    with open(en_file, 'r', encoding='utf-8') as f:\n",
    "        en_lines = f.readlines()\n",
    "    with open(ur_file, 'r', encoding='utf-8') as f:\n",
    "        ur_lines = f.readlines()\n",
    "\n",
    "    # Create dataframe from the two lists\n",
    "    df = pd.DataFrame({'en': en_lines, 'ur': ur_lines})\n",
    "\n",
    "else:\n",
    "  # If not two text files, assume it's a single CSV/TSV with columns to identify\n",
    "  df = pd.read_csv(csv_files[0], encoding='utf-8', error_bad_lines=False)\n",
    "\n",
    "  # You need to identify which columns are English and Urdu; common names: 'english','urdu' or 'en','ur'\n",
    "  # Attempt common guesses\n",
    "  possible_src = [c for c in df.columns if 'en' in c.lower() or 'english' in c.lower()]\n",
    "  possible_tgt = [c for c in df.columns if 'ur' in c.lower() or 'urdu' in c.lower()]\n",
    "  print('possible_src', possible_src)\n",
    "  print('possible_tgt', possible_tgt)\n",
    "\n",
    "\n",
    "  src_col = possible_src[0] if possible_src else df.columns[0]\n",
    "  tgt_col = possible_tgt[0] if possible_tgt else df.columns[1]\n",
    "  print('Using columns:', src_col, tgt_col)\n",
    "\n",
    "\n",
    "  # Keep only those two columns and drop NA rows\n",
    "  df = df[[src_col, tgt_col]].dropna()\n",
    "  df.columns = ['en','ur']\n",
    "\n",
    "\n",
    "print(df.shape)\n",
    "print(df.columns)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e25f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def clean_text_en(s):\n",
    "  s = s.lower()\n",
    "  s=s.strip()\n",
    "  s = re.sub(r\"\\s+\",\" \",s)\n",
    "  return s\n",
    "\n",
    "def clean_text_ur(s):\n",
    "  s=s.lower()\n",
    "  s = str(s).strip()\n",
    "  s = re.sub(r\"\\s+\",\" \",s)\n",
    "  return s\n",
    "\n",
    "_df = df.copy()\n",
    "_df['en'] = _df['en'].apply(clean_text_en)\n",
    "_df['ur'] = _df['ur'].apply(clean_text_ur)\n",
    "\n",
    "\n",
    "# Drop empty lines\n",
    "_df = _df[(_df['en']!='') & (_df['ur']!='')]\n",
    "print('Pairs after cleaning:', len(_df))\n",
    "\n",
    "\n",
    "# Save a master file and show some examples\n",
    "_master = os.path.join(WORKDIR,'parallel_master.csv')\n",
    "_df.to_csv(_master, index=False)\n",
    "print('Saved master csv to', _master)\n",
    "_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b2949c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "train_val, test_df = train_test_split(_df, test_size=0.10, random_state=42)\n",
    "train_df, val_df = train_test_split(train_val, test_size=0.1111, random_state=42) # ~80/10/10\n",
    "\n",
    "\n",
    "print('train, val, test sizes:', len(train_df), len(val_df), len(test_df))\n",
    "\n",
    "\n",
    "for name,dfpart in [('train',train_df),('valid',val_df),('test',test_df)]:\n",
    "  en_path = os.path.join(WORKDIR,f'{name}.en')\n",
    "  ur_path = os.path.join(WORKDIR,f'{name}.ur')\n",
    "  dfpart['en'].to_csv(en_path, index=False, header=False)\n",
    "  dfpart['ur'].to_csv(ur_path, index=False, header=False)\n",
    "  print('Saved:', en_path, ur_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204d3f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import MarianTokenizer, MarianMTModel\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "# ‚úÖ Paths\n",
    "MODEL_DIR = \"/content/drive/MyDrive/mt_en_ur/opus_en_ur_minimal_final\"\n",
    "TEST_FILE = \"/content/drive/MyDrive/mt_en_ur/test.en\"\n",
    "REFERENCE_FILE = \"/content/drive/MyDrive/mt_en_ur/test.ur\"\n",
    "\n",
    "# ‚úÖ Load model and tokenizer\n",
    "print(\"üîÑ Loading fine-tuned model from Drive...\")\n",
    "model = MarianMTModel.from_pretrained(MODEL_DIR)\n",
    "tokenizer = MarianTokenizer.from_pretrained(MODEL_DIR)\n",
    "model = model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"‚úÖ Model loaded on device:\", model.device)\n",
    "\n",
    "# ‚úÖ Load test sentences\n",
    "with open(TEST_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    test_sentences = [line.strip() for line in f.readlines()[:10]]  # only 10 for demo\n",
    "\n",
    "print(\"\\nüìò Example English sentences:\")\n",
    "for i, s in enumerate(test_sentences[:5]):\n",
    "    print(f\"{i+1}. {s}\")\n",
    "\n",
    "# ‚úÖ Translate\n",
    "translated = []\n",
    "for text in test_sentences:\n",
    "    batch = tokenizer([text], return_tensors=\"pt\", padding=True).to(model.device)\n",
    "    generated_tokens = model.generate(**batch)\n",
    "    translation = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0]\n",
    "    translated.append(translation)\n",
    "\n",
    "# ‚úÖ Display results\n",
    "print(\"\\nüåê Translations (English ‚Üí Urdu):\")\n",
    "for en, ur in zip(test_sentences, translated):\n",
    "    print(f\"EN: {en}\")\n",
    "    print(f\"UR: {ur}\")\n",
    "    print(\"‚Äî\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50de781",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sacrebleu.metrics import BLEU\n",
    "\n",
    "# Load references\n",
    "with open(REFERENCE_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    references = [line.strip() for line in f.readlines()[:10]]\n",
    "\n",
    "bleu = BLEU()\n",
    "score = bleu.corpus_score(translated, [references])\n",
    "print(f\"\\nüìä BLEU score on 10 test samples: {score.score:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
